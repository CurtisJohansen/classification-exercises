{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Graph\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import acquire\n",
    "import prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passenger_id</th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   passenger_id  survived  pclass     sex   age  sibsp  parch     fare  \\\n",
       "0             0         0       3    male  22.0      1      0   7.2500   \n",
       "1             1         1       1  female  38.0      1      0  71.2833   \n",
       "\n",
       "  embarked  class deck  embark_town  alone  \n",
       "0        S  Third  NaN  Southampton      0  \n",
       "1        C  First    C    Cherbourg      0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acquire Step\n",
    "df = acquire.get_titanic_data()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19865319865319866"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "177 / 891"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "1. Reduce obvious noise: \n",
    "    - Set `passenger_id` to be the index\n",
    "    - Drop any duplicate columns\n",
    "2. Look into the nulls\n",
    "    - Drop the `deck` column due to so many nulls.\n",
    "    - Fill `embark_town` with the most common value\n",
    "    - `Age` column is ~20% null. I'm not comfortable dropping that many rows, so let's look into age.\n",
    "3. One hot encode our string columns using `pd.get_dummies`\n",
    "4. Split the data into train, validate, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce obvious noise\n",
    "df = df.set_index(\"passenger_id\")\n",
    "df = df.drop(columns=[\"class\", \"embarked\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What about nulls?\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop deck because there are far too many nulls\n",
    "df = df.drop(columns=[\"deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's fill embark_town with the most common observation\n",
    "df.embark_town = df.embark_town.fillna(value=df.embark_town.mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    133\n",
       "0     44\n",
       "Name: alone, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's investigate the observations with missing age\n",
    "# My first thought was empty age values might indicate children\n",
    "# Looks like most of these individuals were traveling alone\n",
    "no_age_info = df[df.age.isna()]\n",
    "no_age_info.alone.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<AxesSubplot:>, <AxesSubplot:>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.fare.hist(), no_age_info.fare.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived\n",
      "Population:\n",
      "0    0.616162\n",
      "1    0.383838\n",
      "Name: survived, dtype: float64\n",
      "No age\n",
      "0    0.706215\n",
      "1    0.293785\n",
      "Name: survived, dtype: float64\n",
      "\n",
      "\n",
      "pclass\n",
      "Population:\n",
      "3    0.551066\n",
      "1    0.242424\n",
      "2    0.206510\n",
      "Name: pclass, dtype: float64\n",
      "No age\n",
      "3    0.768362\n",
      "1    0.169492\n",
      "2    0.062147\n",
      "Name: pclass, dtype: float64\n",
      "\n",
      "\n",
      "sex\n",
      "Population:\n",
      "male      0.647587\n",
      "female    0.352413\n",
      "Name: sex, dtype: float64\n",
      "No age\n",
      "male      0.700565\n",
      "female    0.299435\n",
      "Name: sex, dtype: float64\n",
      "\n",
      "\n",
      "sibsp\n",
      "Population:\n",
      "0    0.682379\n",
      "1    0.234568\n",
      "2    0.031425\n",
      "4    0.020202\n",
      "3    0.017957\n",
      "8    0.007856\n",
      "5    0.005612\n",
      "Name: sibsp, dtype: float64\n",
      "No age\n",
      "0    0.774011\n",
      "1    0.146893\n",
      "8    0.039548\n",
      "3    0.022599\n",
      "2    0.016949\n",
      "Name: sibsp, dtype: float64\n",
      "\n",
      "\n",
      "parch\n",
      "Population:\n",
      "0    0.760943\n",
      "1    0.132435\n",
      "2    0.089787\n",
      "3    0.005612\n",
      "5    0.005612\n",
      "4    0.004489\n",
      "6    0.001122\n",
      "Name: parch, dtype: float64\n",
      "No age\n",
      "0    0.887006\n",
      "2    0.067797\n",
      "1    0.045198\n",
      "Name: parch, dtype: float64\n",
      "\n",
      "\n",
      "embark_town\n",
      "Population:\n",
      "Southampton    0.724409\n",
      "Cherbourg      0.188976\n",
      "Queenstown     0.086614\n",
      "Name: embark_town, dtype: float64\n",
      "No age\n",
      "Southampton    0.508475\n",
      "Queenstown     0.276836\n",
      "Cherbourg      0.214689\n",
      "Name: embark_town, dtype: float64\n",
      "\n",
      "\n",
      "alone\n",
      "Population:\n",
      "1    0.602694\n",
      "0    0.397306\n",
      "Name: alone, dtype: float64\n",
      "No age\n",
      "1    0.751412\n",
      "0    0.248588\n",
      "Name: alone, dtype: float64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how similar this group is to the population\n",
    "for column in df.drop(columns=[\"age\", \"fare\"]).columns:\n",
    "    print(column)\n",
    "    print(\"Population:\")\n",
    "    print(df[column].value_counts(normalize=True))\n",
    "    print(\"No age\")\n",
    "    print(no_age_info[column].value_counts(normalize=True))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the distribution of values, it appears that no age subgroup is very close to the population\n",
    "# If we needed to be more certain, we could perform hypothesis testing\n",
    "# It looks like there's nothing wildly different about the no age group compared to the population\n",
    "# So we'll impute using the median age\n",
    "df.age = df.age.fillna(value=df.age.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding reminder:\n",
    "- Southampton, Queenstown, and Cherbourg were the embark towns\n",
    "- If embark_town_Queenstown and embark_town_Southampton are both 0, then Cherbourg it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>alone</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embark_town_Queenstown</th>\n",
       "      <th>embark_town_Southampton</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>passenger_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              survived  pclass   age  sibsp  parch     fare  alone  sex_male  \\\n",
       "passenger_id                                                                   \n",
       "0                    0       3  22.0      1      0   7.2500      0         1   \n",
       "1                    1       1  38.0      1      0  71.2833      0         0   \n",
       "2                    1       3  26.0      0      0   7.9250      1         0   \n",
       "3                    1       1  35.0      1      0  53.1000      0         0   \n",
       "4                    0       3  35.0      0      0   8.0500      1         1   \n",
       "\n",
       "              embark_town_Queenstown  embark_town_Southampton  \n",
       "passenger_id                                                   \n",
       "0                                  0                        1  \n",
       "1                                  0                        0  \n",
       "2                                  0                        1  \n",
       "3                                  0                        1  \n",
       "4                                  0                        1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time to encode the encodeable!\n",
    "dummy_df = pd.get_dummies(df[['sex','embark_town']], dummy_na=False, drop_first=[True, True])\n",
    "\n",
    "# Drop the original columns we encoded\n",
    "df = df.drop(columns=[\"sex\", \"embark_town\"])\n",
    "\n",
    "# Stitch the df and the dummy_df together again\n",
    "df = pd.concat([df, dummy_df], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to split!\n",
    "train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.survived)\n",
    "train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(columns=[\"survived\"])\n",
    "y_train = train.survived\n",
    "\n",
    "X_validate = validate.drop(columns=[\"survived\"])\n",
    "y_validate = validate.survived\n",
    "\n",
    "X_test = test.drop(columns=[\"survived\"])\n",
    "y_test = test.survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Onto Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy: 0.62\n"
     ]
    }
   ],
   "source": [
    "# The mode is a great baseline\n",
    "baseline = y_train.mode()\n",
    "\n",
    "# Produce a boolean array with True representing a match between the baseline prediction and reality\n",
    "matches_baseline_prediction = y_train == 0\n",
    "\n",
    "baseline_accuracy = matches_baseline_prediction.mean()\n",
    "print(f\"Baseline accuracy: {round(baseline_accuracy, 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree of 1 depth\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.773481</td>\n",
       "      <td>0.801471</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.787476</td>\n",
       "      <td>0.784216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.912052</td>\n",
       "      <td>0.570681</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.741366</td>\n",
       "      <td>0.781124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.837070</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.751868</td>\n",
       "      <td>0.771715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>307.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>498.000000</td>\n",
       "      <td>498.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0           1  accuracy   macro avg  weighted avg\n",
       "precision    0.773481    0.801471  0.781124    0.787476      0.784216\n",
       "recall       0.912052    0.570681  0.781124    0.741366      0.781124\n",
       "f1-score     0.837070    0.666667  0.781124    0.751868      0.771715\n",
       "support    307.000000  191.000000  0.781124  498.000000    498.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the model\n",
    "forest1 = RandomForestClassifier(max_depth=1, random_state=123)\n",
    "\n",
    "# Fit the model (on train and only train)\n",
    "tree1 = forest1.fit(X_train, y_train)\n",
    "\n",
    "# Use the model\n",
    "# We'll evaluate the model's performance on train, first\n",
    "y_predictions = forest1.predict(X_train)\n",
    "\n",
    "# Produce the classification report on the actual y values and this model's predicted y values\n",
    "report = classification_report(y_train, y_predictions, output_dict=True)\n",
    "print(\"Tree of 1 depth\")\n",
    "pd.DataFrame(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get loopy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree with max depth of 2\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.815029    0.835526  0.821285    0.825278      0.822890\n",
      "recall       0.918567    0.664921  0.821285    0.791744      0.821285\n",
      "f1-score     0.863706    0.740525  0.821285    0.802115      0.816462\n",
      "support    307.000000  191.000000  0.821285  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 3\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.836257    0.865385  0.845382    0.850821      0.847429\n",
      "recall       0.931596    0.706806  0.845382    0.819201      0.845382\n",
      "f1-score     0.881356    0.778098  0.845382    0.829727      0.841753\n",
      "support    307.000000  191.000000  0.845382  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 4\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.842566    0.883871  0.855422    0.863218      0.858408\n",
      "recall       0.941368    0.717277  0.855422    0.829323      0.855422\n",
      "f1-score     0.889231    0.791908  0.855422    0.840569      0.851904\n",
      "support    307.000000  191.000000  0.855422  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 5\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.846821    0.907895  0.865462    0.877358      0.870245\n",
      "recall       0.954397    0.722513  0.865462    0.838455      0.865462\n",
      "f1-score     0.897397    0.804665  0.865462    0.851031      0.861831\n",
      "support    307.000000  191.000000  0.865462  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 6\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.866667    0.947712  0.891566    0.907190      0.897750\n",
      "recall       0.973941    0.759162  0.891566    0.866552      0.891566\n",
      "f1-score     0.917178    0.843023  0.891566    0.880101      0.888737\n",
      "support    307.000000  191.000000  0.891566  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 7\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.906907    0.969697  0.927711    0.938302      0.930989\n",
      "recall       0.983713    0.837696  0.927711    0.910705      0.927711\n",
      "f1-score     0.943750    0.898876  0.927711    0.921313      0.926539\n",
      "support    307.000000  191.000000  0.927711  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 8\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.926829    0.982353  0.945783    0.954591      0.948124\n",
      "recall       0.990228    0.874346  0.945783    0.932287      0.945783\n",
      "f1-score     0.957480    0.925208  0.945783    0.941344      0.945103\n",
      "support    307.000000  191.000000  0.945783  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 9\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.938650    0.994186  0.957831    0.966418      0.959950\n",
      "recall       0.996743    0.895288  0.957831    0.946015      0.957831\n",
      "f1-score     0.966825    0.942149  0.957831    0.954487      0.957361\n",
      "support    307.000000  191.000000  0.957831  498.000000    498.000000\n",
      "\n",
      "Tree with max depth of 10\n",
      "                    0           1  accuracy   macro avg  weighted avg\n",
      "precision    0.953416    1.000000   0.96988    0.976708      0.971283\n",
      "recall       1.000000    0.921466   0.96988    0.960733      0.969880\n",
      "f1-score     0.976153    0.959128   0.96988    0.967640      0.969623\n",
      "support    307.000000  191.000000   0.96988  498.000000    498.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's \n",
    "for i in range(2, 11):\n",
    "    # Make the model\n",
    "    forest = RandomForestClassifier(max_depth=i, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    y_predictions = forest.predict(X_train)\n",
    "\n",
    "    # Produce the classification report on the actual y values and this model's predicted y values\n",
    "    report = classification_report(y_train, y_predictions, output_dict=True)\n",
    "    print(f\"Tree with max depth of {i}\")\n",
    "    print(pd.DataFrame(report))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we might expect\n",
    "- The more depth to the tree, the more the model fits to the training data\n",
    "- But we need to expect more to the story!\n",
    "- How well the model works on data it hasn't seen before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.050257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.845382</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.050989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.056356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.865462</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.061724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.078482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.105281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.945783</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.137372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>0.957831</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.154093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>0.969880</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.156795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>0.977912</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.983936</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.170852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.987952</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.188886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.180892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.180892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.993976</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.185565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_depth  train_accuracy  validate_accuracy  difference\n",
       "0           2        0.821285           0.771028    0.050257\n",
       "1           3        0.845382           0.794393    0.050989\n",
       "2           4        0.855422           0.799065    0.056356\n",
       "3           5        0.865462           0.803738    0.061724\n",
       "4           6        0.891566           0.813084    0.078482\n",
       "5           7        0.927711           0.822430    0.105281\n",
       "6           8        0.945783           0.808411    0.137372\n",
       "7           9        0.957831           0.803738    0.154093\n",
       "8          10        0.969880           0.813084    0.156795\n",
       "9          11        0.977912           0.808411    0.169500\n",
       "10         12        0.983936           0.813084    0.170852\n",
       "11         13        0.987952           0.799065    0.188886\n",
       "12         14        0.993976           0.813084    0.180892\n",
       "13         15        0.993976           0.808411    0.185565\n",
       "14         16        0.993976           0.808411    0.185565\n",
       "15         17        0.993976           0.813084    0.180892\n",
       "16         18        0.993976           0.808411    0.185565\n",
       "17         19        0.993976           0.808411    0.185565\n",
       "18         20        0.993976           0.808411    0.185565\n",
       "19         21        0.993976           0.808411    0.185565\n",
       "20         22        0.993976           0.808411    0.185565\n",
       "21         23        0.993976           0.808411    0.185565\n",
       "22         24        0.993976           0.808411    0.185565"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's continue getting loopy, so we can compare in-sample to out-of-sample\n",
    "metrics = []\n",
    "\n",
    "for i in range(2, 25):\n",
    "    # Make the model\n",
    "    forest = RandomForestClassifier(max_depth=i, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.050257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.845382</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.050989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.056356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.865462</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.061724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.078482</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   max_depth  train_accuracy  validate_accuracy  difference\n",
       "0          2        0.821285           0.771028    0.050257\n",
       "1          3        0.845382           0.794393    0.050989\n",
       "2          4        0.855422           0.799065    0.056356\n",
       "3          5        0.865462           0.803738    0.061724\n",
       "4          6        0.891566           0.813084    0.078482"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can modify the above code to set a threshhold of difference\n",
    "# Let's continue getting loopy, so we can compare in-sample to out-of-sample\n",
    "# And this will be a good example of a `break` statement in a loop\n",
    "\n",
    "# Set our threshold for how overfit we'll tolerate\n",
    "threshold = 0.10\n",
    "\n",
    "models = []\n",
    "metrics = []\n",
    "\n",
    "for i in range(2, 25):\n",
    "    # Make the model\n",
    "    forest = RandomForestClassifier(max_depth=i, min_samples_leaf=1, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)   \n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "    # Calculate the difference\n",
    "    difference = in_sample_accuracy - out_of_sample_accuracy\n",
    "    \n",
    "    # Add a conditional to check vs. the threshold\n",
    "    if difference > threshold:\n",
    "        break\n",
    "    \n",
    "    # Formulate the output for each model's performance on train and validate\n",
    "    output = {\n",
    "        \"max_depth\": i,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy,\n",
    "        \"difference\": difference\n",
    "    }\n",
    "    \n",
    "    # Add the metrics dictionary to the list, so we can make a dataframe\n",
    "    metrics.append(output)\n",
    "    \n",
    "    # Add the specific tree to a list of trained models\n",
    "    models.append(forest)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Above Code Replicated the Decision Tree Exercises w/ Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing `min_samples_per_leaf`, decreasing `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_samples_per_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>0.925703</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.103273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0.901606</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.083849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.887550</td>\n",
       "      <td>0.817757</td>\n",
       "      <td>0.069793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.097136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0.871486</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.072421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.869478</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.079758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0.865462</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.071069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.063037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.072383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.064351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.048287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.041606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.831325</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.041606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0.827309</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.046935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.019442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_samples_per_leaf  max_depth  train_accuracy  validate_accuracy  \\\n",
       "0                      2         18        0.925703           0.822430   \n",
       "1                      3         17        0.901606           0.817757   \n",
       "2                      4         16        0.887550           0.817757   \n",
       "3                      5         15        0.877510           0.780374   \n",
       "4                      6         14        0.871486           0.799065   \n",
       "5                      7         13        0.869478           0.789720   \n",
       "6                      8         12        0.865462           0.794393   \n",
       "7                      9         11        0.857430           0.794393   \n",
       "8                     10         10        0.857430           0.785047   \n",
       "9                     11          9        0.849398           0.785047   \n",
       "10                    12          8        0.839357           0.780374   \n",
       "11                    13          7        0.839357           0.780374   \n",
       "12                    14          6        0.833333           0.785047   \n",
       "13                    15          5        0.831325           0.789720   \n",
       "14                    16          4        0.831325           0.789720   \n",
       "15                    17          3        0.827309           0.780374   \n",
       "16                    18          2        0.805221           0.766355   \n",
       "17                    19          1        0.781124           0.761682   \n",
       "\n",
       "    difference  \n",
       "0     0.103273  \n",
       "1     0.083849  \n",
       "2     0.069793  \n",
       "3     0.097136  \n",
       "4     0.072421  \n",
       "5     0.079758  \n",
       "6     0.071069  \n",
       "7     0.063037  \n",
       "8     0.072383  \n",
       "9     0.064351  \n",
       "10    0.058984  \n",
       "11    0.058984  \n",
       "12    0.048287  \n",
       "13    0.041606  \n",
       "14    0.041606  \n",
       "15    0.046935  \n",
       "16    0.038866  \n",
       "17    0.019442  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's continue getting loopy, so we can compare in-sample to out-of-sample\n",
    "metrics = []\n",
    "max_depth = 20\n",
    "\n",
    "for i in range(2, max_depth):\n",
    "    # Make the model\n",
    "    depth = max_depth - i\n",
    "    n_samples = i\n",
    "    forest = RandomForestClassifier(max_depth=depth, min_samples_leaf=n_samples, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"min_samples_per_leaf\": n_samples,\n",
    "        \"max_depth\": depth,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='max_depth', ylabel='difference'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=\"max_depth\", y=\"difference\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='min_samples_per_leaf', ylabel='difference'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=\"min_samples_per_leaf\", y=\"difference\", data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='difference', ylabel='validate_accuracy'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=\"difference\", y=\"validate_accuracy\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if we increase both `min_samples_per_leaf` and `max_depth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_samples_per_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.771028</td>\n",
       "      <td>0.050257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.845382</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.060335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.847390</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.052997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859438</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.060372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.861446</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.062380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.863454</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.073734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.863454</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.073734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.855422</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.061029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.072383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.064351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.054968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.045622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.050295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.044927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.033574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.059640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_samples_per_leaf  max_depth  train_accuracy  validate_accuracy  \\\n",
       "0                      2          2        0.821285           0.771028   \n",
       "1                      3          3        0.845382           0.785047   \n",
       "2                      4          4        0.847390           0.794393   \n",
       "3                      5          5        0.859438           0.799065   \n",
       "4                      6          6        0.861446           0.799065   \n",
       "5                      7          7        0.863454           0.789720   \n",
       "6                      8          8        0.863454           0.789720   \n",
       "7                      9          9        0.855422           0.794393   \n",
       "8                     10         10        0.857430           0.785047   \n",
       "9                     11         11        0.849398           0.785047   \n",
       "10                    12         12        0.839357           0.780374   \n",
       "11                    13         13        0.839357           0.780374   \n",
       "12                    14         14        0.835341           0.780374   \n",
       "13                    15         15        0.835341           0.789720   \n",
       "14                    16         16        0.835341           0.785047   \n",
       "15                    17         17        0.825301           0.780374   \n",
       "16                    18         18        0.823293           0.789720   \n",
       "17                    19         19        0.835341           0.775701   \n",
       "\n",
       "    difference  \n",
       "0     0.050257  \n",
       "1     0.060335  \n",
       "2     0.052997  \n",
       "3     0.060372  \n",
       "4     0.062380  \n",
       "5     0.073734  \n",
       "6     0.073734  \n",
       "7     0.061029  \n",
       "8     0.072383  \n",
       "9     0.064351  \n",
       "10    0.058984  \n",
       "11    0.058984  \n",
       "12    0.054968  \n",
       "13    0.045622  \n",
       "14    0.050295  \n",
       "15    0.044927  \n",
       "16    0.033574  \n",
       "17    0.059640  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's continue getting loopy, so we can compare in-sample to out-of-sample\n",
    "metrics = []\n",
    "max_depth = 20\n",
    "\n",
    "for i in range(2, max_depth):\n",
    "    # Make the model\n",
    "    depth = i\n",
    "    n_samples = i\n",
    "    forest = RandomForestClassifier(max_depth=depth, min_samples_leaf=n_samples, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"min_samples_per_leaf\": n_samples,\n",
    "        \"max_depth\": depth,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about a fixed depth and increasing `min_samples_leaf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_samples_per_leaf</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.921687</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.099257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0.899598</td>\n",
       "      <td>0.808411</td>\n",
       "      <td>0.091187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.887550</td>\n",
       "      <td>0.813084</td>\n",
       "      <td>0.074466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.877510</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.083118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.871486</td>\n",
       "      <td>0.799065</td>\n",
       "      <td>0.072421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0.869478</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.079758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.073077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.063037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.857430</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.072383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.064351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.839357</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.058984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.054968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.045622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.050295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0.825301</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.044927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0.823293</td>\n",
       "      <td>0.789720</td>\n",
       "      <td>0.033574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>0.835341</td>\n",
       "      <td>0.775701</td>\n",
       "      <td>0.059640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.071651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>0.815261</td>\n",
       "      <td>0.785047</td>\n",
       "      <td>0.030214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.780374</td>\n",
       "      <td>0.038903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>0.821285</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.054930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>0.819277</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.052922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.040874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.045547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.043539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.040874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.045547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>0.813253</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.056244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>0.811245</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.054236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.038866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>0.799197</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.037515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.050220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.050220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.050220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.040874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>0.809237</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.042882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>0.809237</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.042882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.040874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>0.807229</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.040874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.034850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>0.805221</td>\n",
       "      <td>0.761682</td>\n",
       "      <td>0.043539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>44</td>\n",
       "      <td>10</td>\n",
       "      <td>0.799197</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.042187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>0.801205</td>\n",
       "      <td>0.766355</td>\n",
       "      <td>0.034850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.026123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>47</td>\n",
       "      <td>10</td>\n",
       "      <td>0.783133</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.026123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.024115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>0.781124</td>\n",
       "      <td>0.757009</td>\n",
       "      <td>0.024115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    min_samples_per_leaf  max_depth  train_accuracy  validate_accuracy  \\\n",
       "0                      2         10        0.921687           0.822430   \n",
       "1                      3         10        0.899598           0.808411   \n",
       "2                      4         10        0.887550           0.813084   \n",
       "3                      5         10        0.877510           0.794393   \n",
       "4                      6         10        0.871486           0.799065   \n",
       "5                      7         10        0.869478           0.789720   \n",
       "6                      8         10        0.867470           0.794393   \n",
       "7                      9         10        0.857430           0.794393   \n",
       "8                     10         10        0.857430           0.785047   \n",
       "9                     11         10        0.849398           0.785047   \n",
       "10                    12         10        0.839357           0.780374   \n",
       "11                    13         10        0.839357           0.780374   \n",
       "12                    14         10        0.835341           0.780374   \n",
       "13                    15         10        0.835341           0.789720   \n",
       "14                    16         10        0.835341           0.785047   \n",
       "15                    17         10        0.825301           0.780374   \n",
       "16                    18         10        0.823293           0.789720   \n",
       "17                    19         10        0.835341           0.775701   \n",
       "18                    20         10        0.833333           0.761682   \n",
       "19                    21         10        0.815261           0.785047   \n",
       "20                    22         10        0.819277           0.780374   \n",
       "21                    23         10        0.821285           0.766355   \n",
       "22                    24         10        0.819277           0.766355   \n",
       "23                    25         10        0.807229           0.766355   \n",
       "24                    26         10        0.807229           0.761682   \n",
       "25                    27         10        0.805221           0.761682   \n",
       "26                    28         10        0.807229           0.766355   \n",
       "27                    29         10        0.807229           0.761682   \n",
       "28                    30         10        0.813253           0.757009   \n",
       "29                    31         10        0.811245           0.757009   \n",
       "30                    32         10        0.805221           0.766355   \n",
       "31                    33         10        0.799197           0.761682   \n",
       "32                    34         10        0.807229           0.757009   \n",
       "33                    35         10        0.807229           0.757009   \n",
       "34                    36         10        0.807229           0.757009   \n",
       "35                    37         10        0.807229           0.766355   \n",
       "36                    38         10        0.809237           0.766355   \n",
       "37                    39         10        0.809237           0.766355   \n",
       "38                    40         10        0.807229           0.766355   \n",
       "39                    41         10        0.807229           0.766355   \n",
       "40                    42         10        0.801205           0.766355   \n",
       "41                    43         10        0.805221           0.761682   \n",
       "42                    44         10        0.799197           0.757009   \n",
       "43                    45         10        0.801205           0.766355   \n",
       "44                    46         10        0.783133           0.757009   \n",
       "45                    47         10        0.783133           0.757009   \n",
       "46                    48         10        0.781124           0.757009   \n",
       "47                    49         10        0.781124           0.757009   \n",
       "\n",
       "    difference  \n",
       "0     0.099257  \n",
       "1     0.091187  \n",
       "2     0.074466  \n",
       "3     0.083118  \n",
       "4     0.072421  \n",
       "5     0.079758  \n",
       "6     0.073077  \n",
       "7     0.063037  \n",
       "8     0.072383  \n",
       "9     0.064351  \n",
       "10    0.058984  \n",
       "11    0.058984  \n",
       "12    0.054968  \n",
       "13    0.045622  \n",
       "14    0.050295  \n",
       "15    0.044927  \n",
       "16    0.033574  \n",
       "17    0.059640  \n",
       "18    0.071651  \n",
       "19    0.030214  \n",
       "20    0.038903  \n",
       "21    0.054930  \n",
       "22    0.052922  \n",
       "23    0.040874  \n",
       "24    0.045547  \n",
       "25    0.043539  \n",
       "26    0.040874  \n",
       "27    0.045547  \n",
       "28    0.056244  \n",
       "29    0.054236  \n",
       "30    0.038866  \n",
       "31    0.037515  \n",
       "32    0.050220  \n",
       "33    0.050220  \n",
       "34    0.050220  \n",
       "35    0.040874  \n",
       "36    0.042882  \n",
       "37    0.042882  \n",
       "38    0.040874  \n",
       "39    0.040874  \n",
       "40    0.034850  \n",
       "41    0.043539  \n",
       "42    0.042187  \n",
       "43    0.034850  \n",
       "44    0.026123  \n",
       "45    0.026123  \n",
       "46    0.024115  \n",
       "47    0.024115  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's continue getting loopy, so we can compare in-sample to out-of-sample\n",
    "metrics = []\n",
    "max_depth = 50\n",
    "\n",
    "for i in range(2, max_depth):\n",
    "    # Make the model\n",
    "    depth = 10\n",
    "    n_samples = i\n",
    "    forest = RandomForestClassifier(max_depth=depth, min_samples_leaf=n_samples, random_state=123)\n",
    "\n",
    "    # Fit the model (on train and only train)\n",
    "    forest = forest.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model\n",
    "    # We'll evaluate the model's performance on train, first\n",
    "    in_sample_accuracy = forest.score(X_train, y_train)\n",
    "    \n",
    "    out_of_sample_accuracy = forest.score(X_validate, y_validate)\n",
    "\n",
    "    output = {\n",
    "        \"min_samples_per_leaf\": n_samples,\n",
    "        \"max_depth\": depth,\n",
    "        \"train_accuracy\": in_sample_accuracy,\n",
    "        \"validate_accuracy\": out_of_sample_accuracy\n",
    "    }\n",
    "    \n",
    "    metrics.append(output)\n",
    "    \n",
    "df = pd.DataFrame(metrics)\n",
    "df[\"difference\"] = df.train_accuracy - df.validate_accuracy\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
